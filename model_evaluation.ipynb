{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ecc7db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6781f105",
   "metadata": {},
   "source": [
    "### 2 Given the following confusion matrix, evaluate (by hand) the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b6458b",
   "metadata": {},
   "source": [
    "#### - in the context of this problem, what is a false positive?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b583b83",
   "metadata": {},
   "source": [
    "#### - in the context of this problem, a false positive would be if an animal was predicted to be a dog, but it was actually a cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ac7054",
   "metadata": {},
   "source": [
    "#### - it was predicted that an animal wasn't a dog, but it actually was"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c845462b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### positive = predicted dog = 46\n",
    "\n",
    "tp  = 46\n",
    "\n",
    "##### negative = predicted cat = 34 \n",
    "\n",
    "tn = 34\n",
    "\n",
    "##### fp = predicted dog but it was a cat\n",
    "\n",
    "fp = 13\n",
    "\n",
    "##### fn = predicted cat but it was a dog \n",
    "\n",
    "fn = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3134e219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = (tp + tn) / ( tp + tn + fp + fn)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4211b2ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8679245283018868"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall  = tp / (tp+fn)\n",
    "recall "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d4c2eb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7796610169491526"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision = tp / (tp + fp)\n",
    "precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97df81ac",
   "metadata": {},
   "source": [
    "### 3 You are working as a datascientist working for Codeup Cody Creator (C3 for short), a rubber-duck manufacturing plant.\n",
    "\n",
    "### Unfortunately, some of the rubber ducks that are produced will have defects. Your team has built several models that try to predict those defects, and the data from their predictions can be found here.\n",
    "\n",
    "### Use the predictions dataset and pandas to help answer the following questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c65780c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46a265ed",
   "metadata": {},
   "source": [
    "#### -An internal team wants to investigate the cause of the manufacturing defects. They tell you that they want to identify as many of the ducks that have a defect as possible. Which evaluation metric would be appropriate here? Which model would be the best fit for this use case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8128dac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      actual     model1  model2     model3\n",
       "0  No Defect  No Defect  Defect  No Defect\n",
       "1  No Defect  No Defect  Defect     Defect\n",
       "2  No Defect  No Defect  Defect  No Defect\n",
       "3  No Defect     Defect  Defect     Defect\n",
       "4  No Defect  No Defect  Defect  No Defect"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"c3.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035b7e4f",
   "metadata": {},
   "source": [
    "-positive = ducks without defects\n",
    "\n",
    "-negative = ducks with defects\n",
    "\n",
    "-false positive = the model says the duck doesn't have defects when it does\n",
    "\n",
    "-false negative = the model says the duck has defects when it doesn't"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd6e5c5",
   "metadata": {},
   "source": [
    "- for this scenario, we would want to use precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a078fb49",
   "metadata": {},
   "source": [
    "#### Recently several stories in the local news have come out highlighting customers who received a rubber duck with a defect, and portraying C3 in a bad light. The PR team has decided to launch a program that gives customers with a defective duck a vacation to Hawaii. They need you to predict which ducks will have defects, but tell you the really don't want to accidentally give out a vacation package when the duck really doesn't have a defect. Which evaluation metric would be appropriate here? Which model would be the best fit for this use case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9c67c4",
   "metadata": {},
   "source": [
    "- here i think we would want to use the recall method to get the actual defective count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b6161c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No Defect    184\n",
       "Defect        16\n",
       "Name: actual, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.actual.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a183b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
